# -*- coding: utf-8 -*-
"""
Created on Mon Nov 14 15:47:32 2022

@author: Michael

#References:
# https://github.com/jkrusina/SoccerPredictor
# https://github.com/mhaythornthwaite/Football_Prediction_Project
"""




#!/usr/bin/python
from os.path import dirname, realpath, sep, pardir
import sys, os

sys.path.append(dirname(realpath(__file__)) + sep + pardir + sep)
#os.getcwd()

import time
start=time.time()

from ml_functions.ml_model_eval import pred_proba_plot, plot_cross_val_confusion_matrix, plot_learning_curve
from ml_functions.data_processing import scale_df
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, accuracy_score
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
import pandas as pd

plt.close('all')


#INPUT VARIABLES 

df_5_saved_name = '2019_2020_2021_2022_prem_df_for_ml_5_v2.txt'
df_10_saved_name = '2019_2020_2021_2022_prem_df_for_ml_10_v2.txt'

grid_search = False

pred_prob_plot_df10 = False
save_pred_prob_plot_df10 = False
pred_prob_plot_df5 = True
save_pred_prob_plot_df5 = False

save_conf_matrix_df10 = False
save_conf_matrix_df5 = False

save_learning_curve_df10 = False
save_learning_curve_df5 = False

create_final_model = False


# ML MODEL BUILD

#importing the data and creating the feature dataframe and target series

with open('C:/Users/Michael/Documents/MACF courses material/Fall 2022/STAT 497/Final Project/Template code/Football_Prediction_Project/prem_clean_fixtures_and_dataframes/2019_2020_2021_2022_prem_df_for_ml_5_v2.txt', 'rb') as myFile:
    df_ml_5 = pickle.load(myFile)

with open('C:/Users/Michael/Documents/MACF courses material/Fall 2022/STAT 497/Final Project/Template code/Football_Prediction_Project/prem_clean_fixtures_and_dataframes/2019_2020_2021_2022_prem_df_for_ml_10_v2.txt', 'rb') as myFile:
    df_ml_10 = pickle.load(myFile)
    
    
#scaling dataframe to make all features to have zero mean and unit vector.
df_ml_10 = scale_df(df_ml_10, list(range(14)), [14,15,16])
df_ml_5 = scale_df(df_ml_5, list(range(14)), [14,15,16])

x_10 = df_ml_10.drop(['Fixture ID', 'Team Result Indicator', 'Opponent Result Indicator', 'Team Av Shots Diff', 
                    'Opponent Av Shots Diff', 'Team Av Possession Diff', 'Opponent Av Possession Diff'], axis=1)
y_10 = df_ml_10['Team Result Indicator']

x_5 = df_ml_5.drop(['Fixture ID', 'Team Result Indicator', 'Opponent Result Indicator'], axis=1)
y_5 = df_ml_5['Team Result Indicator']


matrix = x_10.corr().round(2)
sns.heatmap(matrix, annot=True, vmax=1, vmin=-1, center=0, cmap='vlag')
plt.show()

#------------------------------- XgBoost--------------------------------

## DMatrix

def XGboost_train(df, print_result=True, print_result_label=''):

    #create features matrix
    x = df.drop(['Fixture ID', 'Team Result Indicator', 'Opponent Result Indicator', 'Team Av Shots Diff', 
                    'Opponent Av Shots Diff', 'Team Av Possession Diff', 'Opponent Av Possession Diff'], axis=1)
    y = df['Team Result Indicator']

  

    clf = xgb.XGBClassifier( objective ='multi:softprob', subsample =0.6 ,colsample_bytree = 0.8, learning_rate = 0.1, 
                max_depth = 7, max_leaves = 1, reg_lambda =7 , min_child_weight =5, 
                reg_alpha  = 11, n_estimators=50,  seed =479)
   

    
    #split into training data and test data
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

    clf.fit(x_train, y_train)
    
    if print_result:
        print(print_result_label)
        #training data
        train_data_score = round(clf.score(x_train, y_train) * 100, 1)
        print(f'Training data score = {train_data_score}%')
        
        #test data
        test_data_score = round(clf.score(x_test, y_test) * 100, 1)
        print(f'Test data score = {test_data_score}% \n')
    
    return clf, x_train, x_test, y_train, y_test


ml_10_xgBoosting, x10_train, x10_test, y10_train, y10_test = XGboost_train(df_ml_10, print_result_label='DF_ML_10')
ml_5_xgBoosting, x5_train, x5_test, y5_train, y5_test = XGboost_train(df_ml_5, print_result_label='DF_ML_5')

# ---------- ENSEMBLE MODELLING ----------

#In this section we will combine the results of using the same algorithm but with different input data used to train the model. The features are still broadly the same but have been averaged over a different number of games df_ml_10 is 10 games, df_ml_5 is 5 games. 

#reducing fixtures in df_ml_5 to contain only the fixtures within df_ml_10 and training that new dataset
df_ml_5_dropto10 = df_ml_5.drop(list(range(0,50)))
ml_5_to10_xgBoost, x5_to10_train, x5_to10_test, y5_to10_train, y5_to10_test = XGboost_train(df_ml_5_dropto10, print_result=False)

#making predictions using the two df inputs independantly
y_pred_ml10 = ml_10_xgBoosting.predict(x10_test)
y_pred_ml5to10 = ml_5_to10_xgBoost.predict(x10_test)

#making probability predictions on each of the datasets independantly
pred_proba_ml10 = ml_10_xgBoosting.predict_proba(x10_test)
pred_proba_ml5_10 = ml_5_to10_xgBoost.predict_proba(x10_test)

#combining independant probabilities and creating combined class prediction
pred_proba_ml5and10 = (np.array(pred_proba_ml10) + np.array(pred_proba_ml5_10)) / 2.0
y_pred_ml5and10 = np.argmax(pred_proba_ml5and10, axis=1)

#accuracy score variables
y_pred_ml10_accuracy = round(accuracy_score(y10_test, y_pred_ml10), 3) * 100
y_pred_ml5to10_accuracy = round(accuracy_score(y10_test, y_pred_ml5to10), 3) * 100
y_pred_ml5and10_accuracy = round(accuracy_score(y10_test, y_pred_ml5and10), 3) * 100

print('ENSEMBLE MODEL TESTING')
print(f'Accuracy of df_10 alone = {y_pred_ml10_accuracy}%')
print(confusion_matrix(y10_test, y_pred_ml10), '\n')
print(f'Accuracy of df_5 alone = {y_pred_ml5to10_accuracy}%')
print(confusion_matrix(y10_test, y_pred_ml5to10), '\n')
print(f'Accuracy of df_5 and df_10 combined = {y_pred_ml5and10_accuracy}%')
print(confusion_matrix(y10_test, y_pred_ml5and10), '\n\n')


# ---------- GRID & RANDOM SEARCH  ----------
# search for optimal hyperparameters
if grid_search:
    
    params = {
        'learning_rate':[0.1,0.05,0.01],
        'reg_alpha': np.arange(1, 20, step=1),
        'reg_lambda': np.arange(1, 20, step=1),
        'max_leaves': list(np.arange(1, 10, step=1)) + [None],
        'max_depth':  list(np.arange(1, 10, step=1)) + [None],
        'n_estimators': np.arange(50, 150, step=50) ,
        'early_stopping_rounds': [3,5,10]
        }
    
    random_serch =RandomizedSearchCV(ml_10_xgBoosting, 
                                     param_distributions=params,
                                     n_iter= 500,
                                     n_jobs= -1, 
                                     cv=5, 
                                     verbose=3, 
                                     random_state=1001 )

    params = {
        'learning_rate':[0.1,0.05,0.01],
        'reg_alpha':[10, 13, 15],
        'reg_lambda':[10, 13, 15],
        'min_child_weight': [ 5, 10,15],
        'max_leaves': [6, 8, 10],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.8, 1.0, 1.2],
        'max_depth': [3, 4, 5],
        'n_estimators': [ 40, 50, 80 ] #,
        #'early_stopping_rounds': [3,5,10]
        }

    grid_search_grad = GridSearchCV(ml_10_xgBoosting, 
                                    params, 
                                    cv= 5, 
                                     n_jobs= -1,
                                    verbose = 3,
                                    scoring = 'accuracy', 
                                    return_train_score = True)
    random_serch.fit(x_10, y_10)
    

    print('\n', 'Gradient Best Params: ' , random_serch.best_params_)
    print('Gradient Best Score: ' , random_serch.best_score_ , '\n')


#------------------------------- MODEL EVALUATION -----------------------------

#Stratified K-Fold Cross validation
skf = StratifiedKFold(n_splits=5, shuffle=True)

cv_score_av = round(np.mean(cross_val_score(ml_10_xgBoosting, x_10, y_10, cv=skf))*100,1)
print('Cross-Validation Accuracy Score ML10: ', cv_score_av, '%\n')

cv_score_av = round(np.mean(cross_val_score(ml_5_xgBoosting, x_5, y_5, cv=skf))*100,1)
print('Cross-Validation Accuracy Score ML5: ', cv_score_av, '%\n')


# ---------- PREDICTION PROBABILITY PLOTS ----------

if pred_prob_plot_df10:
    fig = pred_proba_plot(ml_10_xgBoosting, 
                          x_10, 
                          y_10, 
                          no_iter=50, 
                          no_bins=36, 
                          x_min=0.3, 
                          classifier='XgBoost (ml_10)')
    if save_pred_prob_plot_df10:
        fig.savefig('figures/ml_10_xgBoost_pred_proba.png')

if pred_prob_plot_df5:
    fig = pred_proba_plot(ml_5_xgBoosting, 
                          x_5, 
                          y_5, 
                          no_iter=50, 
                          no_bins=35, 
                          x_min=0.3, 
                          classifier='XgBoost (ml_5)')
    if save_pred_prob_plot_df5:
        fig.savefig('figures/ml_5_xgBoost_pred_proba.png')


# ---------- CONFUSION MATRIX PLOTS ----------

#modified to take cross-val results.

plot_cross_val_confusion_matrix(ml_10_xgBoosting, 
                                x_10, 
                                y_10, 
                                display_labels=('team loses', 'draw', 'team wins'), 
                                title='XgBoost Confusion Matrix ML10', 
                                cv=skf)
if save_conf_matrix_df10:
    plt.savefig('figures\ml_10_confusion_matrix_cross_val_xgboost.png')

plot_cross_val_confusion_matrix(ml_5_xgBoosting, 
                                x_5, 
                                y_5, 
                                display_labels=('team loses', 'draw', 'team wins'), 
                                title='XgBoostConfusion Matrix ML5', 
                                cv=skf)
if save_conf_matrix_df5:
    plt.savefig('figures\ml_5_confusion_matrix_cross_val_xgboost.png')


# ---------- LEARNING CURVE PLOTS ----------

plot_learning_curve(ml_10_xgBoosting, 
                    x_10, 
                    y_10, 
                    training_set_size=20, 
                    x_max=600, 
                    title='Learning Curve - XgBoostDF_10')
if save_learning_curve_df10:
    plt.savefig('figures\ml_10_xgboost_learning_curve.png')

plot_learning_curve(ml_5_xgBoosting, 
                    x_5, 
                    y_5, 
                    training_set_size=20, 
                    x_max=600, 
                    title='Learning Curve - XgBoostDF_5')
if save_learning_curve_df5:
    plt.savefig('figures\ml_5_xgboost_learning_curve.png')


# ---------- FEATURE IMPORTANCE ----------

fi_ml_10 = pd.DataFrame({'feature': list(x10_train.columns),'importance': ml_10_xgBoosting.feature_importances_}).sort_values('importance', ascending = False)

fi_ml_5 = pd.DataFrame({'feature': list(x5_train.columns),'importance': ml_5_xgBoosting.feature_importances_}).sort_values('importance', ascending = False)

print('Feature Importance for ML10: \n', fi_ml_10)    
print('Feature Importance for ML5: \n', fi_ml_5)
    
# Feature Importance: Team Av Shots Inside Box Diff       13.06%
# Feature Importance: Opponent Av Goal Diff               10.97%
# Feature Importance: Team Av Pass Accuracy Diff          10.54%
# Feature Importance: Opponent Av Shots Diff              9.79%
# Feature Importance: Team Av Shots Diff                  8.89%
# Feature Importance: Opponent Av Corners Diff            6.63%
# Feature Importance: Team Av Possession Diff             6.48%
# Feature Importance: Opponent Av Pass Accuracy Diff      6.15%
# Feature Importance: Team Av Goal Diff                   5.18%
# Feature Importance: Opponent Av Fouls Diff              4.96%
# Feature Importance: Opponent Av Shots Inside Box Diff   4.8%
# Feature Importance: Team Av Fouls Diff                  4.52%
# Feature Importance: Opponent Av Possession Diff         4.2%
# Feature Importance: Team Av Corners Diff                3.83%


#--------------------------------- FINAL MODEL --------------------------------

#in this section we will take the learnings from the hyperparameter testing above and train a final model using 100% of the data. This model may then be used for predictions going forward.

if create_final_model:
    
    #intantiating and training the df_5 network
    ml_5_rf = xgb.XGBClassifier( objective ='multi:softprob', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 120, seed =479)
    ml_5_rf.fit(x_5, y_5)
    
    #intantiating and training the df_10 network
    ml_10_rf = xgb.XGBClassifier( objective ='multi:softprob', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 120, seed =479)
    ml_10_rf.fit(x_10, y_10)
    
    with open('C:/Users/Michael/Documents/MACF courses material/Fall 2022/STAT 497/Final Project/Template code/Football_Prediction_Project/ml_model_build_xgBoost/xgboost_model_5.pk1', 'wb') as myFile:
        pickle.dump(ml_5_rf, myFile)

    with open('C:/Users/Michael/Documents/MACF courses material/Fall 2022/STAT 497/Final Project/Template code/Football_Prediction_Project/ml_model_build_xgBoost/xgboost_model_10.pk1', 'wb') as myFile:
       pickle.dump(ml_10_rf, myFile)

